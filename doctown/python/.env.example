# Embedding Model Configuration
# Copy this file to .env and customize as needed

# Model Selection (choose one approach)
# ---------------------------------------

# Option 1: Use a preset name
EMBEDDING_MODEL_PRESET=fast
# Available presets: fast, balanced, quality, multilingual, code

# Option 2: Use any HuggingFace model ID (overrides preset)
# EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
# EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
# EMBEDDING_MODEL=intfloat/e5-large-v2
# EMBEDDING_MODEL=microsoft/codebert-base

# Cache & Device Configuration
# ---------------------------------------

# Where to cache downloaded models
EMBEDDING_CACHE_DIR=./models/embeddings

# Device to use: cuda, cpu, or auto
EMBEDDING_DEVICE=auto

# HuggingFace Authentication
# ---------------------------------------

# Only needed for private/gated models
# HF_TOKEN=hf_your_token_here

# OpenAI Configuration (for LLM-based documentation)
# ---------------------------------------

# Your OpenAI API key (required for LLM docs)
# OPENAI_API_KEY=sk-your-key-here

# Model to use (gpt-5-nano is fastest/cheapest)
OPENAI_MODEL=gpt-5-nano

# Optional: Custom API base URL (for Azure, proxies, etc.)
# OPENAI_BASE_URL=https://your-azure-endpoint.openai.azure.com

# Quick Reference
# ---------------------------------------
# List models:     python scripts/download_model.py --list
# Download:        python scripts/download_model.py --preset fast
# Test:            python examples/test_embedder.py
# Documentation:   see EMBEDDING_MODELS.md
